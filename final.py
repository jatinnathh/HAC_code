# -*- coding: utf-8 -*-
"""HAC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1owOULzZHR_VdU8S3oW2iUOXBHoA2o5oA
"""

!git clone https://github.com/jatinnathh/data.git

!pip install pathway sentence-transformers torch pandas

import pathway as pw
from pathway.stdlib.ml.index import KNNIndex
from sentence_transformers import SentenceTransformer
import pandas as pd
from pathlib import Path

import pathway as pw

# 1. Read as BINARY to grab the whole file at once
#    (Unlike plaintext, this does not split on new lines)
raw_data = pw.io.fs.read(
    path="/content/data/Books/",
    format="binary",
    mode="static",
    with_metadata=True
)

# 2. Decode the bytes into a String
#    We use a UDF to convert the raw bytes (b'Chapter 1...') into text.
@pw.udf
def decode_file(data: bytes) -> str:
    try:
        return data.decode('utf-8')
    except:
        return data.decode('latin-1') # Fallback for older books

@pw.udf
def as_string(val) -> str:
    return str(val)

documents = raw_data.select(
    text=decode_file(pw.this.data),
    doc_id=as_string(pw.this._metadata["path"])
)

# 3. VERIFICATION: Check the size now
#    (We convert to pandas just for this print statement)
df = pw.debug.table_to_pandas(documents)

print(f"--- READ COMPLETE ---")
print(f"Total Books Loaded: {len(df)}") # Should be 2 (one row per file)

if not df.empty:
    for index, row in df.iterrows():
        print(f"\nBook: {row['doc_id']}")
        print(f"Length: {len(row['text'])} characters") # Should be > 500,000
        print("Start of text:", row['text'][:100].replace('\n', ' '))

df

print(len(df['text'][0]))
print(df['doc_id'][0])

!pip install -U langchain langchain-community langchain-experimental

import pathway as pw
import re
import pandas as pd
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings

# --- 1. SETUP EMBEDDINGS (Global Model) ---
# Initialize once to save time
semantic_embedder = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# --- 2. UDFs (User Defined Functions) ---

@pw.udf
def decode_binary(data: bytes) -> str:
    """Decodes file bytes to string with fallback."""
    try:
        return data.decode("utf-8")
    except Exception:
        return data.decode("latin-1", errors="ignore")

@pw.udf
def split_chapters_indexed(text: str) -> list[tuple[int, str]]:
    """
    Splits text into chapters AND returns their order index.
    Returns: [(1, "Chapter 1 text..."), (2, "Chapter 2 text...")]
    """
    if not text: return []

    # Normalize headers
    text = re.sub(r"\n{3,}", "\n\n", text)
    # Split on Roman Numeral or Digit Headers
    parts = re.split(r"\n\s*(CHAPTER\s+[IVXLCDM0-9]+.*?)\n", text, flags=re.IGNORECASE)

    chapters = []
    # If no chapters found, treat whole book as Chapter 1
    if len(parts) < 2:
        return [(1, text)]

    current_idx = 1

    # Handle Prologue (text before first chapter header)
    if parts[0].strip():
        chapters.append((current_idx, parts[0]))
        current_idx += 1

    # Loop through parts (Header + Body)
    for i in range(1, len(parts), 2):
        header = parts[i]
        body = parts[i+1] if i+1 < len(parts) else ""
        chapter_content = header + "\n" + body
        chapters.append((current_idx, chapter_content))
        current_idx += 1

    return chapters

@pw.udf
def semantic_chunk_chapter(chapter_text: str) -> list[tuple[str, dict]]:
    """
    Semantic chunking wrapper.
    Returns: list of (chunk_text, metadata_dict)
    """
    # 1. Clean input
    if not chapter_text: return []
    clean_text = re.sub(r"\n{3,}", "\n\n", chapter_text)

    # 2. Semantic Split
    try:
        splitter = SemanticChunker(
            semantic_embedder,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=90
        )
        docs = splitter.create_documents([clean_text])
        raw_chunks = [d.page_content for d in docs]
    except:
        # Fallback if model fails on weird chars
        raw_chunks = [clean_text]

    # 3. Simple Size Guard (Merge very small chunks)
    # (Simplified for brevity, but keeps logic valid)
    final_chunks = []
    buffer = ""
    min_chars = 300

    for chunk in raw_chunks:
        if len(buffer) + len(chunk) < min_chars:
            buffer += " " + chunk
        else:
            final_chunks.append(buffer.strip() if buffer else chunk)
            buffer = chunk
    if buffer: final_chunks.append(buffer)

    # 4. Return with index
    return [
        (text, {"strategy": "semantic", "chunk_index": i})
        for i, text in enumerate(final_chunks)
    ]

import pathway as pw
import re
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings

# --- 1. ROBUST UDFS ---

@pw.udf
def get_chunk_index_safe(metadata: dict) -> int:
    # Robustly grabs the index, handling strings or ints
    val = metadata.get("chunk_index", 0)
    try:
        return int(val)
    except:
        return 0

@pw.udf
def semantic_chunk_chapter_strict(chapter_text: str) -> list[tuple[str, dict]]:
    if not chapter_text: return []

    # A. Semantic Split
    try:
        splitter = SemanticChunker(
            semantic_embedder,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=90
        )
        docs = splitter.create_documents([chapter_text])
        raw_chunks = [d.page_content for d in docs]
    except:
        raw_chunks = [chapter_text]

    # B. SIZE POLICING (The Fix)
    final_chunks = []
    MAX_CHARS = 1500  # Safe limit for MiniLM

    for chunk in raw_chunks:
        # If chunk is massive, force-split it by paragraphs or newlines
        if len(chunk) > MAX_CHARS:
            # Simple recursive split by double newline, then single newline
            sub_parts = re.split(r'\n\n|\n', chunk)
            buffer = ""
            for part in sub_parts:
                if len(buffer) + len(part) < MAX_CHARS:
                    buffer += "\n" + part
                else:
                    if buffer.strip(): final_chunks.append(buffer.strip())
                    buffer = part
            if buffer.strip(): final_chunks.append(buffer.strip())
        else:
            final_chunks.append(chunk)

    # C. Return with Index
    return [
        (text, {"chunk_index": i})
        for i, text in enumerate(final_chunks)
        if len(text) > 50  # Filter out noise (<50 chars)
    ]

import pathway as pw

# --- CORRECTED UDF (No .get(), uses brackets) ---
@pw.udf
def get_chunk_index_safe(metadata: dict) -> int:
    try:
        # Pathway Json objects support ['key'] but often not .get()
        return int(metadata["chunk_index"])
    except:
        # If key is missing or anything fails, return 0 to keep going
        return 0

# --- RE-RUN PIPELINE ---
print("â³ Running Pipeline (Attempt 4 - Fixed UDF)...")

# We don't need to redefine the whole pipeline function if you only updated the UDF
# and the pipeline calls 'get_chunk_index_safe' by name.
# However, to be safe, let's re-declare the pipeline ensuring it uses the new UDF.

def run_ingestion_pipeline(data_dir="/content/data/Books/"):
    # 1. Read & Decode
    raw = pw.io.fs.read(data_dir, format="binary", mode="static", with_metadata=True)
    documents = raw.select(
        path=pw.this._metadata["path"],
        text=decode_binary(pw.this.data)
    )

    # 2. Split Chapters
    chapters = documents.select(
        path=pw.this.path,
        chapter_data=split_chapters_indexed(pw.this.text)
    ).flatten(pw.this.chapter_data)

    # 3. Strict Semantic Chunking
    chunked = chapters.select(
        path=pw.this.path,
        chapter_idx=pw.this.chapter_data[0],
        chunks=semantic_chunk_chapter_strict(pw.this.chapter_data[1])
    )

    # 4. Flatten & Rank (Using the FIXED UDF)
    final_chunks = chunked.flatten(pw.this.chunks).select(
        path=pw.this.path,
        chunk_text=pw.this.chunks[0],

        # Calling the new safe UDF
        global_rank=(pw.this.chapter_idx * 100000) + get_chunk_index_safe(pw.this.chunks[1]),

        chapter_id=pw.this.chapter_idx
    )

    return final_chunks

# Execute
pipeline = run_ingestion_pipeline()
df = pw.debug.table_to_pandas(pipeline)

print(f"âœ… Pipeline complete. Total chunks: {len(df)}")

print(type(df.iloc[0]["path"]))
print(df.iloc[0]["path"])

df["path_str"] = df["path"].astype(str)
print(df.iloc[0]["path_str"])

print(questions_raw.select(pw.this.book_name).schema)
print(
    pw.debug.table_to_pandas(
        questions_raw.select(pw.this.book_name)
    ).head(10)
)

# 1. SANITIZE: Convert Pathway types to Python types
# This fixes the "Json" sorting error
df['path_str'] = df['path'].astype(str)
df['global_rank'] = df['global_rank'].astype(int)

# 2. SORT: Now it is safe to sort
df_sorted = df.sort_values(by=['path_str', 'global_rank'], ascending=[True, True])

# 3. VERIFY QUALITY
print(f"âœ… Dataframe Sorted. Total Rows: {len(df_sorted)}")

print("\nðŸ”Ž RANK CHECK (First 5 IDs - MUST NOT be all the same):")
print(df_sorted['global_rank'].head(10).tolist())

print("\nðŸ“ LENGTH CHECK:")
print(f"Max Length: {df_sorted['chunk_text'].str.len().max()} (Target: ~1500)")
print(f"Min Length: {df_sorted['chunk_text'].str.len().min()} (Target: > 50)")

print("\nðŸ“– NARRATIVE FLOW CHECK (The 'Seam' Test):")
# Check a random transition in the middle
middle_idx = 100
row_a = df_sorted.iloc[middle_idx]
row_b = df_sorted.iloc[middle_idx + 1]

if row_a['path_str'] == row_b['path_str']:
    print(f"Chunk {row_a['global_rank']} Ends: ...{str(row_a['chunk_text'])[-50:].replace(chr(10), ' ')}")
    print(f"Chunk {row_b['global_rank']} Starts: {str(row_b['chunk_text'])[:50].replace(chr(10), ' ')}...")
else:
    print("Hit a book boundary, try a different index.")

# --- SETUP ---
# Ensure we work with the sorted dataframe
df['path_str'] = df['path'].astype(str)
df_sorted = df.sort_values(by=['path_str', 'global_rank'], ascending=[True, True])

# --- TEST ---
print("ðŸ•µï¸ NARRATIVE FLOW CHECK")
print("I will print the 'seam' between 3 random consecutive chunks.")
print("Read them. If the sentence cuts off in Chunk A and finishes in Chunk B, IT IS WORKING.\n")

# Pick a random starting point in the middle of the book
start_idx = 100

for i in range(start_idx, start_idx + 3):
    current = df_sorted.iloc[i]
    next_chunk = df_sorted.iloc[i+1]

    print(f"ðŸ”¹ Chunk {current['global_rank']} (End):")
    print(f"...{current['chunk_text'][-150:].replace(chr(10), ' ')}")
    print(f"ðŸ”¸ Chunk {next_chunk['global_rank']} (Start):")
    print(f"{next_chunk['chunk_text'][:150].replace(chr(10), ' ')}...")
    print("-" * 60)

df.columns

import matplotlib.pyplot as plt

# Calculate lengths
df['char_len'] = df['chunk_text'].str.len()

print("\nðŸ“Š CHUNK STATISTICS")
print(df['char_len'].describe())

# Histogram
plt.figure(figsize=(10, 4))
df['char_len'].hist(bins=50, color='skyblue', edgecolor='black')
plt.title('Distribution of Chunk Lengths')
plt.xlabel('Characters')
plt.ylabel('Count')
plt.show()

print("\nðŸ“š BOOK SEPARATION CHECK")
print(df_sorted['path_str'].value_counts())

# Check for duplicate ranks (This should be 0)
duplicates = df_sorted.duplicated(subset=['path_str', 'global_rank']).sum()
print(f"\nâŒ Duplicate Ranks found: {duplicates}")
# If this is > 0, we have a problem.

@pw.udf
def normalize_book_name(name) -> str:
    s = str(name).lower()
    s = s.replace(".txt", "")
    s = re.sub(r"[^a-z0-9\s]", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

import pathway as pw
from pathway.stdlib.ml.index import KNNIndex

############################################
# 1. INGEST SEMANTIC CHUNKS
############################################

print("â³ Building semantic chunk pipeline...")
pipeline = run_ingestion_pipeline()

############################################
# 2. SAFE PATH CLEANING (Json-safe)
############################################

@pw.udf
def clean_path(path) -> str:
    s = str(path)
    if not s:
        return ""
    return s.split("/")[-1]

pipeline_clean = pipeline.select(
    chunk_text=pw.this.chunk_text,
    path=pw.this.path,
    book_name=clean_path(pw.this.path)
)

############################################
# 3. EMBEDD DOCUMENT CHUNKS
############################################

@pw.udf
def embed_text(text: str) -> list[float]:
    if not text:
        return [0.0] * 384
    return semantic_embedder.embed_query(text)

pipeline_with_vectors = pipeline_clean.with_columns(
    vector=embed_text(pw.this.chunk_text)
)

############################################
# 4. BUILD VECTOR INDEX
############################################

print("â³ Creating vector index...")
index = KNNIndex(
    data=pipeline_with_vectors,
    data_embedding=pipeline_with_vectors.vector,
    n_dimensions=384
)

############################################
# 5. LOAD QUESTIONS
############################################

questions_schema = pw.schema_from_csv("/content/data/train.csv")

questions_raw = pw.io.csv.read(
    "/content/data/train.csv",
    schema=questions_schema,
    mode="static"
)

print("Columns found:", list(questions_raw.keys()))

questions = questions_raw.select(
    question_id=pw.this.id,
    book_name=pw.this.book_name,
    char=pw.this.char,
    caption=pw.this.caption,
    label=pw.this.label,
    backstory=pw.this.content
)

print("New schema:", list(questions.keys()))

############################################
# 6. EMBED BACKSTORIES (QUERY VECTORS)
############################################

query_table = questions.select(
    question_id=pw.this.question_id,
    narrative=pw.this.book_name,
    backstory=pw.this.backstory,
    label=pw.this.label,
    vector=embed_text(pw.this.backstory)
)

############################################
# 7. RETRIEVE TOP-K CHUNKS
############################################

TOP_K = 10

nearest_results = index.get_nearest_items(
    query_embedding=query_table.vector,
    k=TOP_K
)

############################################
# 8. JOIN QUERY + RESULTS
############################################

query_meta = query_table.select(
    question_id=pw.this.question_id,
    narrative=pw.this.narrative,
    backstory=pw.this.backstory,
    label=pw.this.label
)

nearest = query_meta + nearest_results

debug_df = pw.debug.table_to_pandas(
    nearest.select(
        narrative=pw.this.narrative,
        found_book=clean_path(pw.this.book_name)
    )
)

print(debug_df.head(10))


############################################
# 9. FILTER BY SAME BOOK
############################################

filtered = nearest.select(
    question_id=pw.this.question_id,
    narrative_raw=pw.this.narrative,
    narrative_norm=normalize_book_name(pw.this.narrative),
    found_book_norm=normalize_book_name(clean_path(pw.this.path)),
    backstory=pw.this.backstory,
    label=pw.this.label,
    chunk_text=pw.this.chunk_text
).filter(
    pw.this.narrative_norm == pw.this.found_book_norm
)

print("Before filter:", len(debug_df))
print("After filter:", len(pw.debug.table_to_pandas(filtered)))



############################################
# 10. GROUP EVIDENCE PER QUESTION
############################################

final_output = filtered.groupby(pw.this.question_id).reduce(
    question_id=pw.this.question_id,
    narrative=pw.reducers.any(pw.this.narrative_raw),
    backstory=pw.reducers.any(pw.this.backstory),
    label=pw.reducers.any(pw.this.label),
    valid_chunks=pw.reducers.tuple(pw.this.chunk_text)
)

############################################
# 11. EXECUTE
############################################

print("â³ Running retrieval...")
df = pw.debug.table_to_pandas(final_output)

print(f"âœ… Retrieval complete. Matches found for {len(df)} questions.")

if not df.empty:
    row = df.iloc[0]
    print("\nðŸ“˜ Book:", row["narrative"])
    print("ðŸ§  Backstory:", row["backstory"][:120], "...\n")

    chunks = row["valid_chunks"]
    print(f"âœ… Evidence chunks: {len(chunks)}")
    print("\nðŸ“œ Sample chunk:\n", chunks[0][:400], "...")

print(df.head(1))



"""# SIMPLE BASELINE"""

import pandas as pd
import re
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def normalize_label(x):
    if isinstance(x, str):
        return 1 if x.lower().startswith("consist") else 0
    return int(x)

df = pw.debug.table_to_pandas(final_output)

df["y_true"] = df["label"].apply(normalize_label)

def predict_option_a(row):
    chunks = row["valid_chunks"]
    if chunks and len(chunks) > 0:
        return 1
    return 0

df["pred_a"] = df.apply(predict_option_a, axis=1)

train_df, val_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df["y_true"]
)

train_acc = accuracy_score(train_df["y_true"], train_df["pred_a"])
val_acc = accuracy_score(val_df["y_true"], val_df["pred_a"])

print("Option A â€” Train Accuracy:", train_acc)
print("Option A â€” Val Accuracy:", val_acc)

train_df

val_df

"""# CLAIM-LEVEL REASONING

"""

def flatten_chunks(valid_chunks):
    """
    valid_chunks can be:
    - None
    - ()
    - ((chunk1, chunk2, ...),)
    - (chunk1, chunk2, ...)
    """
    if not valid_chunks:
        return []

    flattened = []

    for item in valid_chunks:
        if isinstance(item, str):
            flattened.append(item)
        elif isinstance(item, (list, tuple)):
            for sub in item:
                if isinstance(sub, str):
                    flattened.append(sub)

    return flattened

def split_into_claims(backstory: str):
    sentences = re.split(r"[.;]\s+", backstory)
    return [s.strip() for s in sentences if len(s.strip()) > 20]

def score_claim_with_reason(claim: str, evidence_chunks: list[str]):
    claim_words = set(claim.lower().split())

    for chunk in evidence_chunks:
        chunk_lower = chunk.lower()
        chunk_words = set(chunk_lower.split())

        # CONTRADICTION
        negations = {"never", "no", "not", "none", "without"}
        if negations & set(chunk_lower.split()) and len(claim_words & chunk_words) > 1:
            return -1, f"Contradicted by narrative evidence opposing claim: '{claim}'."

        # SUPPORT
        if len(claim_words & chunk_words) >= 3:
            return +1, f"Supported by narrative evidence related to claim: '{claim}'."

    return 0, f"No direct narrative evidence found for claim: '{claim}'."

def aggregate_scores(scores):
    neg = scores.count(-1)
    pos = scores.count(+1)

    if neg >= 2:
        return 0
    if pos >= 1 and neg == 0:
        return 1
    return 0

def predict_option_b(row):
    claims = split_into_claims(row["backstory"])
    evidence = flatten_chunks(row["valid_chunks"])

    scores = [score_claim(c, evidence) for c in claims]
    return aggregate_scores(scores)

df["pred_b"] = df.apply(predict_option_b, axis=1)

train_df, val_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df["y_true"]
)

train_acc = accuracy_score(train_df["y_true"], train_df["pred_b"])
val_acc = accuracy_score(val_df["y_true"], val_df["pred_b"])

print("Option B â€” Train Accuracy:", train_acc)
print("Option B â€” Val Accuracy:", val_acc)

# Ensure evidence is strings
sample = df.iloc[0]
evidence = flatten_chunks(sample["valid_chunks"])
print(type(evidence), type(evidence[0]))

df[(df["pred_a"] == 1) & (df["pred_b"] == 0) & (df["y_true"] == 0)][
    ["backstory", "valid_chunks"]
].head(3)

from sklearn.model_selection import StratifiedKFold
import numpy as np

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

accs = []

for train_idx, val_idx in skf.split(df, df["y_true"]):
    train_df = df.iloc[train_idx]
    val_df = df.iloc[val_idx]

    acc = accuracy_score(val_df["y_true"], val_df["pred_b"])
    accs.append(acc)

print("Option B â€” CV Accuracy:", np.mean(accs))
print("Std:", np.std(accs))

def generate_rationale_option_b(row):
    claims = split_into_claims(row["backstory"])
    evidence = flatten_chunks(row["valid_chunks"])

    scored = [score_claim_with_reason(c, evidence) for c in claims]

    # Priority: contradiction > support > neutral
    for score, reason in scored:
        if score == -1:
            return reason

    for score, reason in scored:
        if score == +1:
            return reason

    return "The narrative provides insufficient evidence to support the proposed backstory."

df["rationale"] = df.apply(generate_rationale_option_b, axis=1)

df[[
    "backstory",
    "y_true",     # âœ… correct label
    "pred_b",     # ðŸ”® your prediction
    "rationale"   # ðŸ§  explanation
]].head(5)

from sklearn.metrics import accuracy_score

acc = accuracy_score(df["y_true"], df["pred_b"])
print("Option B â€” Accuracy:", acc)

print("Total samples:", len(df))
print("Correct predictions:", (df["y_true"] == df["pred_b"]).sum())
print("Incorrect predictions:", (df["y_true"] != df["pred_b"]).sum())

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(df["y_true"], df["pred_b"])
print("Confusion Matrix:\n", cm)

